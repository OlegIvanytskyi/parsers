{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'pets' : 'familie/haustiere', 'politics' : 'politik',\n",
    "              'economy' : 'wirtschaft', 'travel' : 'konsum/reisen',\n",
    "              'health' : 'gesundheit/medizin-krankheit', 'food' : 'ernahrung/lebensmittel',\n",
    "              'parenting' : 'familie/erziehung'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with pets category\n",
      "Number of pages = 10\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Finished working with pets category. Scraped 107 articles\n",
      "\n",
      "107 articles of pets category have been scraped from beobachter_ch\n",
      "\n",
      "__________________________________________________________________________\n",
      "Working with politics category\n",
      "Number of pages = 16\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Finished working with politics category. Scraped 181 articles\n",
      "\n",
      "181 articles of politics category have been scraped from beobachter_ch\n",
      "\n",
      "__________________________________________________________________________\n",
      "Working with economy category\n",
      "Number of pages = 8\n",
      "Scraped 50 articles\n",
      "Finished working with economy category. Scraped 89 articles\n",
      "\n",
      "89 articles of economy category have been scraped from beobachter_ch\n",
      "\n",
      "__________________________________________________________________________\n",
      "Working with travel category\n",
      "Number of pages = 24\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Finished working with travel category. Scraped 278 articles\n",
      "\n",
      "278 articles of travel category have been scraped from beobachter_ch\n",
      "\n",
      "__________________________________________________________________________\n",
      "Working with health category\n",
      "Number of pages = 49\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Finished working with health category. Scraped 571 articles\n",
      "\n",
      "571 articles of health category have been scraped from beobachter_ch\n",
      "\n",
      "__________________________________________________________________________\n",
      "Working with food category\n",
      "Number of pages = 8\n",
      "Scraped 50 articles\n",
      "Finished working with food category. Scraped 95 articles\n",
      "\n",
      "95 articles of food category have been scraped from beobachter_ch\n",
      "\n",
      "__________________________________________________________________________\n",
      "Working with parenting category\n",
      "Number of pages = 23\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Finished working with parenting category. Scraped 262 articles\n",
      "\n",
      "262 articles of parenting category have been scraped from beobachter_ch\n",
      "\n",
      "__________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for category, path in categories.items():\n",
    "    print(f'Working with {category} category')\n",
    "\n",
    "    articles = []\n",
    "    page_num = 1\n",
    "\n",
    "    while True:\n",
    "        url = f'https://www.beobachter.ch/{path}?page={page_num}#page'\n",
    "        page = requests.get(url)    \n",
    "\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        if page_num == 1:\n",
    "            try:\n",
    "                limit = int(soup.find('div', class_='_2fQ8qDhH _1deHe_oF').\n",
    "                            find_all('a', class_='page-loader-btn ShimA1UP')[-1].text)\n",
    "                print(f'Number of pages = {limit}')\n",
    "            except AttributeError:\n",
    "                limit = 1\n",
    "                \n",
    "        \n",
    "        divs = soup.find_all('div', class_='_23Qt4XY-')\n",
    "\n",
    "        if divs == []:\n",
    "            break\n",
    "\n",
    "        for div in divs[1:]:\n",
    "            sub_divs = div.find_all('a', class_='_2p02b0mw teaser-m-default')\n",
    "\n",
    "            for sub_div in sub_divs:\n",
    "                teaser_headline = sub_div.find('div', class_='_2QZpcT0M _3du5gVzi _3iAkatGy')\n",
    "                teaser = sub_div.find('div', class_='_2QcjsKyb M6MeFidn')\n",
    "\n",
    "                if None in (teaser_headline, teaser):\n",
    "                    continue\n",
    "\n",
    "                teaser_headline = teaser_headline.text.strip()\n",
    "                teaser = teaser.text.strip()[:-5] #[:-5] to throw away the word \"Mehr\"\n",
    "                articles.append([teaser_headline, teaser])\n",
    "\n",
    "                if len(articles) % 50 == 0:\n",
    "                    print(f'Scraped {len(articles)} articles')\n",
    "\n",
    "        page_num += 1\n",
    "        if page_num > limit:\n",
    "            break\n",
    "            \n",
    "    print(f'Finished working with {category} category. Scraped {len(articles)} articles\\n')\n",
    "    write_to_file(articles, category, 'beobachter_ch')\n",
    "    print('__________________________________________________________________________')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
