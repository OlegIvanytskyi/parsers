{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'pets' : 'familie/haustiere', 'politics' : 'politik',\n",
    "              'economy' : 'wirtschaft', 'travel' : 'konsum/reisen',\n",
    "              'health' : 'gesundheit/medizin-krankheit', 'food' : 'ernahrung/lebensmittel',\n",
    "              'parenting' : 'familie/erziehung', 'realestate' : 'wohnen/eigentum',\n",
    "              'career' : 'arbeit', 'home' : 'wohnen/bauen-renovieren',\n",
    "              'nature' : 'umwelt/umweltpolitik'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with pets category\n",
      "Number of pages = 10\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 105 lines\n",
      "Scraped 141 lines\n",
      "Scraped 177 lines\n",
      "Scraped 213 lines\n",
      "Scraped 249 lines\n",
      "Scraped 285 lines\n",
      "Scraped 321 lines\n",
      "Scraped 324 lines\n",
      "Finished working with pets category. Scraped 324 lines\n",
      "\n",
      "324 lines of pets category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with politics category\n",
      "Number of pages = 17\n",
      "Scraped 24 lines\n",
      "Scraped 60 lines\n",
      "Scraped 96 lines\n",
      "Scraped 132 lines\n",
      "Scraped 168 lines\n",
      "Scraped 204 lines\n",
      "Scraped 240 lines\n",
      "Scraped 276 lines\n",
      "Scraped 312 lines\n",
      "Scraped 348 lines\n",
      "Scraped 384 lines\n",
      "Scraped 420 lines\n",
      "Scraped 456 lines\n",
      "Scraped 483 lines\n",
      "Scraped 504 lines\n",
      "Scraped 540 lines\n",
      "Scraped 549 lines\n",
      "Finished working with politics category. Scraped 549 lines\n",
      "\n",
      "549 lines of politics category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with economy category\n",
      "Number of pages = 8\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 144 lines\n",
      "Scraped 180 lines\n",
      "Scraped 216 lines\n",
      "Scraped 252 lines\n",
      "Scraped 282 lines\n",
      "Finished working with economy category. Scraped 282 lines\n",
      "\n",
      "282 lines of economy category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with travel category\n",
      "Number of pages = 24\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 144 lines\n",
      "Scraped 180 lines\n",
      "Scraped 216 lines\n",
      "Scraped 252 lines\n",
      "Scraped 288 lines\n",
      "Scraped 324 lines\n",
      "Scraped 360 lines\n",
      "Scraped 396 lines\n",
      "Scraped 432 lines\n",
      "Scraped 465 lines\n",
      "Scraped 501 lines\n",
      "Scraped 537 lines\n",
      "Scraped 570 lines\n",
      "Scraped 606 lines\n",
      "Scraped 642 lines\n",
      "Scraped 678 lines\n",
      "Scraped 714 lines\n",
      "Scraped 750 lines\n",
      "Scraped 786 lines\n",
      "Scraped 822 lines\n",
      "Scraped 840 lines\n",
      "Finished working with travel category. Scraped 840 lines\n",
      "\n",
      "840 lines of travel category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with health category\n",
      "Number of pages = 50\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 144 lines\n",
      "Scraped 180 lines\n",
      "Scraped 216 lines\n",
      "Scraped 252 lines\n",
      "Scraped 288 lines\n",
      "Scraped 324 lines\n",
      "Scraped 360 lines\n",
      "Scraped 396 lines\n",
      "Scraped 432 lines\n",
      "Scraped 468 lines\n",
      "Scraped 504 lines\n",
      "Scraped 540 lines\n",
      "Scraped 576 lines\n",
      "Scraped 612 lines\n",
      "Scraped 648 lines\n",
      "Scraped 684 lines\n",
      "Scraped 720 lines\n",
      "Scraped 756 lines\n",
      "Scraped 792 lines\n",
      "Scraped 828 lines\n",
      "Scraped 864 lines\n",
      "Scraped 900 lines\n",
      "Scraped 936 lines\n",
      "Scraped 972 lines\n",
      "Scraped 1002 lines\n",
      "Scraped 1038 lines\n",
      "Scraped 1062 lines\n",
      "Scraped 1095 lines\n",
      "Scraped 1131 lines\n",
      "Scraped 1161 lines\n",
      "Scraped 1197 lines\n",
      "Scraped 1233 lines\n",
      "Scraped 1269 lines\n",
      "Scraped 1305 lines\n",
      "Scraped 1335 lines\n",
      "Scraped 1371 lines\n",
      "Scraped 1404 lines\n",
      "Scraped 1440 lines\n",
      "Scraped 1476 lines\n",
      "Scraped 1512 lines\n",
      "Scraped 1548 lines\n",
      "Scraped 1584 lines\n",
      "Scraped 1617 lines\n",
      "Scraped 1653 lines\n",
      "Scraped 1689 lines\n",
      "Scraped 1725 lines\n",
      "Scraped 1734 lines\n",
      "Finished working with health category. Scraped 1734 lines\n",
      "\n",
      "1734 lines of health category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with food category\n",
      "Number of pages = 8\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 144 lines\n",
      "Scraped 177 lines\n",
      "Scraped 213 lines\n",
      "Scraped 249 lines\n",
      "Scraped 285 lines\n",
      "Finished working with food category. Scraped 285 lines\n",
      "\n",
      "285 lines of food category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with parenting category\n",
      "Number of pages = 23\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 138 lines\n",
      "Scraped 174 lines\n",
      "Scraped 210 lines\n",
      "Scraped 246 lines\n",
      "Scraped 282 lines\n",
      "Scraped 318 lines\n",
      "Scraped 354 lines\n",
      "Scraped 390 lines\n",
      "Scraped 426 lines\n",
      "Scraped 462 lines\n",
      "Scraped 498 lines\n",
      "Scraped 534 lines\n",
      "Scraped 570 lines\n",
      "Scraped 606 lines\n",
      "Scraped 642 lines\n",
      "Scraped 678 lines\n",
      "Scraped 714 lines\n",
      "Scraped 747 lines\n",
      "Scraped 783 lines\n",
      "Scraped 786 lines\n",
      "Finished working with parenting category. Scraped 786 lines\n",
      "\n",
      "786 lines of parenting category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with realestate category\n",
      "Number of pages = 7\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 141 lines\n",
      "Scraped 174 lines\n",
      "Scraped 207 lines\n",
      "Scraped 234 lines\n",
      "Finished working with realestate category. Scraped 234 lines\n",
      "\n",
      "234 lines of realestate category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with career category\n",
      "Number of pages = 68\n",
      "Scraped 36 lines\n",
      "Scraped 66 lines\n",
      "Scraped 102 lines\n",
      "Scraped 138 lines\n",
      "Scraped 174 lines\n",
      "Scraped 210 lines\n",
      "Scraped 246 lines\n",
      "Scraped 282 lines\n",
      "Scraped 318 lines\n",
      "Scraped 354 lines\n",
      "Scraped 390 lines\n",
      "Scraped 426 lines\n",
      "Scraped 462 lines\n",
      "Scraped 498 lines\n",
      "Scraped 534 lines\n",
      "Scraped 570 lines\n",
      "Scraped 606 lines\n",
      "Scraped 642 lines\n",
      "Scraped 678 lines\n",
      "Scraped 714 lines\n",
      "Scraped 750 lines\n",
      "Scraped 777 lines\n",
      "Scraped 804 lines\n",
      "Scraped 837 lines\n",
      "Scraped 873 lines\n",
      "Scraped 909 lines\n",
      "Scraped 942 lines\n",
      "Scraped 975 lines\n",
      "Scraped 1011 lines\n",
      "Scraped 1047 lines\n",
      "Scraped 1083 lines\n",
      "Scraped 1119 lines\n",
      "Scraped 1155 lines\n",
      "Scraped 1191 lines\n",
      "Scraped 1224 lines\n",
      "Scraped 1260 lines\n",
      "Scraped 1296 lines\n",
      "Scraped 1329 lines\n",
      "Scraped 1359 lines\n",
      "Scraped 1395 lines\n",
      "Scraped 1428 lines\n",
      "Scraped 1461 lines\n",
      "Scraped 1491 lines\n",
      "Scraped 1524 lines\n",
      "Scraped 1548 lines\n",
      "Scraped 1578 lines\n",
      "Scraped 1611 lines\n",
      "Scraped 1647 lines\n",
      "Scraped 1680 lines\n",
      "Scraped 1716 lines\n",
      "Scraped 1752 lines\n",
      "Scraped 1788 lines\n",
      "Scraped 1824 lines\n",
      "Scraped 1854 lines\n",
      "Scraped 1887 lines\n",
      "Scraped 1923 lines\n",
      "Scraped 1959 lines\n",
      "Scraped 1995 lines\n",
      "Scraped 2031 lines\n",
      "Scraped 2064 lines\n",
      "Scraped 2097 lines\n",
      "Scraped 2130 lines\n",
      "Scraped 2166 lines\n",
      "Scraped 2199 lines\n",
      "Scraped 2232 lines\n",
      "Scraped 2259 lines\n",
      "Scraped 2280 lines\n",
      "Scraped 2298 lines\n",
      "Finished working with career category. Scraped 2298 lines\n",
      "\n",
      "2298 lines of career category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with home category\n",
      "Number of pages = 10\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 141 lines\n",
      "Scraped 177 lines\n",
      "Scraped 213 lines\n",
      "Scraped 246 lines\n",
      "Scraped 282 lines\n",
      "Scraped 318 lines\n",
      "Scraped 330 lines\n",
      "Finished working with home category. Scraped 330 lines\n",
      "\n",
      "330 lines of home category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with nature category\n",
      "Number of pages = 13\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 144 lines\n",
      "Scraped 180 lines\n",
      "Scraped 216 lines\n",
      "Scraped 252 lines\n",
      "Scraped 288 lines\n",
      "Scraped 324 lines\n",
      "Scraped 360 lines\n",
      "Scraped 396 lines\n",
      "Scraped 432 lines\n",
      "Scraped 468 lines\n",
      "Finished working with nature category. Scraped 468 lines\n",
      "\n",
      "468 lines of nature category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "website = 'https://www.beobachter.ch'\n",
    "\n",
    "for category, path in categories.items():\n",
    "    print(f'Working with {category} category')\n",
    "\n",
    "    lines = []\n",
    "    page_num = 1\n",
    "\n",
    "    while True:\n",
    "        url = f'{website}/{path}?page={page_num}#page'\n",
    "        page = requests.get(url)    \n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        if page_num == 1:\n",
    "            try:\n",
    "                limit = int(soup.find('div', class_='_2fQ8qDhH _1deHe_oF').\n",
    "                            find_all('a', class_='page-loader-btn ShimA1UP')[-1].text)\n",
    "            except AttributeError:\n",
    "                limit = 1\n",
    "            print(f'Number of pages = {limit}')\n",
    "            \n",
    "        divs = soup.find_all('div', class_='_23Qt4XY-')\n",
    "\n",
    "        if divs == []:\n",
    "            break\n",
    "\n",
    "        for div in divs[1:]:\n",
    "            sub_divs = div.find_all('a', class_='_2p02b0mw teaser-m-default')\n",
    "\n",
    "            for sub_div in sub_divs:\n",
    "                teaser_headline = sub_div.find('div', class_='_2QZpcT0M _3du5gVzi _3iAkatGy')\n",
    "                teaser = sub_div.find('div', class_='_2QcjsKyb M6MeFidn')\n",
    "                href = sub_div['href']\n",
    "                \n",
    "                # going to the page of the article to get keywords\n",
    "                try:\n",
    "                    article = requests.get(website + href)\n",
    "                    article_soup = BeautifulSoup(article.content, 'html.parser')\n",
    "                    keywords = ' '.join(article_soup.find('meta', attrs={'name':'news_keywords'})['content'].split(','))\n",
    "                except TypeError:\n",
    "                    keywords = ''\n",
    "                    \n",
    "                if None in (teaser_headline, teaser):\n",
    "                    continue\n",
    "\n",
    "                teaser_headline = teaser_headline.text.strip()\n",
    "                teaser = teaser.text.strip()[:-5] #[:-5] to throw away the word \"Mehr\"\n",
    "                \n",
    "                lines.append(teaser_headline)\n",
    "                lines.append(teaser)\n",
    "                lines.append(keywords)\n",
    "\n",
    "        print(f'Scraped {len(lines)} lines')\n",
    "\n",
    "        page_num += 1\n",
    "        if page_num > limit:\n",
    "            break\n",
    "            \n",
    "    print(f'Finished working with {category} category. Scraped {len(lines)} lines\\n')\n",
    "    write_to_file(lines, category, 'beobachter_ch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
