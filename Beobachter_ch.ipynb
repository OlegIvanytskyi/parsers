{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'pets' : 'familie/haustiere', 'politics' : 'politik',\n",
    "              'economy' : 'wirtschaft', 'travel' : 'konsum/reisen',\n",
    "              'health' : 'gesundheit/medizin-krankheit', 'food' : 'ernahrung/lebensmittel',\n",
    "              'parenting' : 'familie/erziehung'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with pets category\n",
      "Number of pages = 10\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Finished working with pets category. Scraped 214 articles\n",
      "\n",
      "214 articles of pets category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with politics category\n",
      "Number of pages = 16\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Finished working with politics category. Scraped 362 articles\n",
      "\n",
      "362 articles of politics category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with economy category\n",
      "Number of pages = 8\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Finished working with economy category. Scraped 178 articles\n",
      "\n",
      "178 articles of economy category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with travel category\n",
      "Number of pages = 24\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Finished working with travel category. Scraped 554 articles\n",
      "\n",
      "554 articles of travel category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with health category\n",
      "Number of pages = 49\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Scraped 600 articles\n",
      "Scraped 650 articles\n",
      "Scraped 700 articles\n",
      "Scraped 750 articles\n",
      "Scraped 800 articles\n",
      "Scraped 850 articles\n",
      "Scraped 900 articles\n",
      "Scraped 950 articles\n",
      "Scraped 1000 articles\n",
      "Scraped 1050 articles\n",
      "Scraped 1100 articles\n",
      "Finished working with health category. Scraped 1140 articles\n",
      "\n",
      "1140 articles of health category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with food category\n",
      "Number of pages = 8\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Finished working with food category. Scraped 190 articles\n",
      "\n",
      "190 articles of food category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with parenting category\n",
      "Number of pages = 23\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Finished working with parenting category. Scraped 524 articles\n",
      "\n",
      "524 articles of parenting category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for category, path in categories.items():\n",
    "    print(f'Working with {category} category')\n",
    "\n",
    "    articles = []\n",
    "    page_num = 1\n",
    "\n",
    "    while True:\n",
    "        url = f'https://www.beobachter.ch/{path}?page={page_num}#page'\n",
    "        page = requests.get(url)    \n",
    "\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        if page_num == 1:\n",
    "            try:\n",
    "                limit = int(soup.find('div', class_='_2fQ8qDhH _1deHe_oF').\n",
    "                            find_all('a', class_='page-loader-btn ShimA1UP')[-1].text)\n",
    "                print(f'Number of pages = {limit}')\n",
    "            except AttributeError:\n",
    "                limit = 1\n",
    "                \n",
    "        \n",
    "        divs = soup.find_all('div', class_='_23Qt4XY-')\n",
    "\n",
    "        if divs == []:\n",
    "            break\n",
    "\n",
    "        for div in divs[1:]:\n",
    "            sub_divs = div.find_all('a', class_='_2p02b0mw teaser-m-default')\n",
    "\n",
    "            for sub_div in sub_divs:\n",
    "                teaser_headline = sub_div.find('div', class_='_2QZpcT0M _3du5gVzi _3iAkatGy')\n",
    "                teaser = sub_div.find('div', class_='_2QcjsKyb M6MeFidn')\n",
    "\n",
    "                if None in (teaser_headline, teaser):\n",
    "                    continue\n",
    "\n",
    "                teaser_headline = teaser_headline.text.strip()\n",
    "                teaser = teaser.text.strip()[:-5] #[:-5] to throw away the word \"Mehr\"\n",
    "                \n",
    "                articles.append(teaser_headline)\n",
    "                articles.append(teaser)\n",
    "\n",
    "                if len(articles) % 50 == 0:\n",
    "                    print(f'Scraped {len(articles)} articles')\n",
    "\n",
    "        page_num += 1\n",
    "        if page_num > limit:\n",
    "            break\n",
    "            \n",
    "    print(f'Finished working with {category} category. Scraped {len(articles)} articles\\n')\n",
    "    write_to_file(articles, category, 'beobachter_ch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
