{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'pets' : 'familie/haustiere', 'politics' : 'politik',\n",
    "              'economy' : 'wirtschaf', 'travel' : 'konsum/reisen',\n",
    "              'health' : 'gesundheit/medizin-krankheit', 'food' : 'ernahrung/lebensmittel',\n",
    "              'parenting' : 'familie/erziehung'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with pets category\n",
      "10\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Finished working with pets category. Scraped 107 articles\n",
      "\n",
      "\n",
      "107 articles of pets category have been scraped from beobachter_ch\n",
      "\n",
      "Working with politics category\n",
      "16\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Finished working with politics category. Scraped 181 articles\n",
      "\n",
      "\n",
      "181 articles of politics category have been scraped from beobachter_ch\n",
      "\n",
      "Working with economy category\n",
      "Finished working with economy category. Scraped 0 articles\n",
      "\n",
      "\n",
      "0 articles of economy category have been scraped from beobachter_ch\n",
      "\n",
      "Working with travel category\n",
      "24\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Finished working with travel category. Scraped 106 articles\n",
      "\n",
      "\n",
      "106 articles of travel category have been scraped from beobachter_ch\n",
      "\n",
      "Working with health category\n",
      "49\n",
      "Finished working with health category. Scraped 36 articles\n",
      "\n",
      "\n",
      "36 articles of health category have been scraped from beobachter_ch\n",
      "\n",
      "Working with food category\n",
      "8\n",
      "Finished working with food category. Scraped 12 articles\n",
      "\n",
      "\n",
      "12 articles of food category have been scraped from beobachter_ch\n",
      "\n",
      "Working with parenting category\n",
      "23\n",
      "Finished working with parenting category. Scraped 12 articles\n",
      "\n",
      "\n",
      "12 articles of parenting category have been scraped from beobachter_ch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category, path in categories.items():\n",
    "    print(f'Working with {category} category')\n",
    "\n",
    "    articles = []\n",
    "    page_num = 1\n",
    "\n",
    "    while True:\n",
    "        url = f'https://www.beobachter.ch/{path}?page={page_num}#page'\n",
    "        page = requests.get(url)    \n",
    "\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        if page_num == 1:\n",
    "            try:\n",
    "                limit = int(soup.find('div', class_='_2fQ8qDhH _1deHe_oF').\n",
    "                            find_all('a', class_='page-loader-btn ShimA1UP')[-1].text)\n",
    "                print(limit)\n",
    "            except AttributeError:\n",
    "                limit = 1\n",
    "                \n",
    "        \n",
    "        divs = soup.find_all('div', class_='_23Qt4XY-')\n",
    "\n",
    "        if divs == []:\n",
    "            break\n",
    "\n",
    "        for div in divs[1:]:\n",
    "            sub_divs = div.find_all('a', class_='_2p02b0mw teaser-m-default')\n",
    "\n",
    "            for sub_div in sub_divs:\n",
    "                teaser_headline = sub_div.find('div', class_='_2QZpcT0M _3du5gVzi _3iAkatGy')\n",
    "                teaser = sub_div.find('div', class_='_2QcjsKyb M6MeFidn')\n",
    "\n",
    "                if None in (teaser_headline, teaser):\n",
    "                    continue\n",
    "\n",
    "                teaser_headline = teaser_headline.text.strip()\n",
    "                teaser = teaser.text.strip()[:-5] #[:-5] to throw away the word \"Mehr\"\n",
    "                articles.append([teaser_headline, teaser])\n",
    "\n",
    "                if len(articles) % 50 == 0:\n",
    "                    print(f'Scraped {len(articles)} articles')\n",
    "\n",
    "        page_num += 1\n",
    "        if page_num > limit:\n",
    "            break\n",
    "            \n",
    "    print(f'Finished working with {category} category. Scraped {len(articles)} articles\\n\\n')\n",
    "    write_to_file(articles, category, 'beobachter_ch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
