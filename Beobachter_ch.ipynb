{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'pets' : 'familie/haustiere', 'politics' : 'politik',\n",
    "              'economy' : 'wirtschaft', 'travel' : 'konsum/reisen',\n",
    "              'health' : 'gesundheit/medizin-krankheit', 'food' : 'ernahrung/lebensmittel',\n",
    "              'parenting' : 'familie/erziehung', 'realestate' : 'wohnen/eigentum',\n",
    "              'career' : 'arbeit', 'home' : 'wohnen/bauen-renovieren',\n",
    "              'nature' : 'umwelt/umweltpolitik'}\n",
    "website = 'https://www.beobachter.ch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with pets category\n",
      "Number of pages = 10\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 105 lines\n",
      "Scraped 141 lines\n",
      "Scraped 177 lines\n",
      "Scraped 213 lines\n",
      "Scraped 249 lines\n",
      "Scraped 285 lines\n",
      "Scraped 321 lines\n",
      "Scraped 324 lines\n",
      "Finished working with pets category. Scraped 324 lines\n",
      "\n",
      "324 lines of pets category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with politics category\n",
      "Number of pages = 17\n",
      "Scraped 24 lines\n",
      "Scraped 60 lines\n",
      "Scraped 96 lines\n",
      "Scraped 132 lines\n",
      "Scraped 168 lines\n",
      "Scraped 204 lines\n",
      "Scraped 240 lines\n",
      "Scraped 276 lines\n",
      "Scraped 312 lines\n",
      "Scraped 348 lines\n",
      "Scraped 384 lines\n",
      "Scraped 420 lines\n",
      "Scraped 456 lines\n",
      "Scraped 486 lines\n",
      "Scraped 504 lines\n",
      "Scraped 540 lines\n",
      "Scraped 552 lines\n",
      "Finished working with politics category. Scraped 552 lines\n",
      "\n",
      "552 lines of politics category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with economy category\n",
      "Number of pages = 8\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 144 lines\n",
      "Scraped 180 lines\n",
      "Scraped 216 lines\n",
      "Scraped 252 lines\n",
      "Scraped 282 lines\n",
      "Finished working with economy category. Scraped 282 lines\n",
      "\n",
      "282 lines of economy category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with travel category\n",
      "Number of pages = 24\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 144 lines\n",
      "Scraped 180 lines\n",
      "Scraped 216 lines\n",
      "Scraped 252 lines\n",
      "Scraped 288 lines\n",
      "Scraped 324 lines\n",
      "Scraped 360 lines\n",
      "Scraped 396 lines\n",
      "Scraped 432 lines\n",
      "Scraped 465 lines\n",
      "Scraped 501 lines\n",
      "Scraped 537 lines\n",
      "Scraped 570 lines\n",
      "Scraped 606 lines\n",
      "Scraped 642 lines\n",
      "Scraped 678 lines\n",
      "Scraped 714 lines\n",
      "Scraped 750 lines\n",
      "Scraped 786 lines\n",
      "Scraped 822 lines\n",
      "Scraped 840 lines\n",
      "Finished working with travel category. Scraped 840 lines\n",
      "\n",
      "840 lines of travel category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with health category\n",
      "Number of pages = 50\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 144 lines\n",
      "Scraped 180 lines\n",
      "Scraped 216 lines\n",
      "Scraped 252 lines\n",
      "Scraped 288 lines\n",
      "Scraped 324 lines\n",
      "Scraped 360 lines\n",
      "Scraped 396 lines\n",
      "Scraped 432 lines\n",
      "Scraped 468 lines\n",
      "Scraped 504 lines\n",
      "Scraped 540 lines\n",
      "Scraped 576 lines\n",
      "Scraped 612 lines\n",
      "Scraped 648 lines\n",
      "Scraped 684 lines\n",
      "Scraped 720 lines\n",
      "Scraped 756 lines\n",
      "Scraped 792 lines\n",
      "Scraped 828 lines\n",
      "Scraped 864 lines\n",
      "Scraped 900 lines\n",
      "Scraped 936 lines\n",
      "Scraped 972 lines\n",
      "Scraped 1005 lines\n",
      "Scraped 1041 lines\n",
      "Scraped 1068 lines\n",
      "Scraped 1101 lines\n",
      "Scraped 1137 lines\n",
      "Scraped 1167 lines\n",
      "Scraped 1203 lines\n",
      "Scraped 1239 lines\n",
      "Scraped 1275 lines\n",
      "Scraped 1311 lines\n",
      "Scraped 1341 lines\n",
      "Scraped 1377 lines\n",
      "Scraped 1410 lines\n",
      "Scraped 1446 lines\n",
      "Scraped 1482 lines\n",
      "Scraped 1518 lines\n",
      "Scraped 1554 lines\n",
      "Scraped 1590 lines\n",
      "Scraped 1623 lines\n",
      "Scraped 1659 lines\n",
      "Scraped 1695 lines\n",
      "Scraped 1731 lines\n",
      "Scraped 1746 lines\n",
      "Finished working with health category. Scraped 1746 lines\n",
      "\n",
      "1746 lines of health category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with food category\n",
      "Number of pages = 8\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 144 lines\n",
      "Scraped 177 lines\n",
      "Scraped 213 lines\n",
      "Scraped 249 lines\n",
      "Scraped 285 lines\n",
      "Finished working with food category. Scraped 285 lines\n",
      "\n",
      "285 lines of food category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with parenting category\n",
      "Number of pages = 23\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 138 lines\n",
      "Scraped 174 lines\n",
      "Scraped 210 lines\n",
      "Scraped 246 lines\n",
      "Scraped 282 lines\n",
      "Scraped 318 lines\n",
      "Scraped 354 lines\n",
      "Scraped 390 lines\n",
      "Scraped 426 lines\n",
      "Scraped 462 lines\n",
      "Scraped 498 lines\n",
      "Scraped 534 lines\n",
      "Scraped 570 lines\n",
      "Scraped 606 lines\n",
      "Scraped 642 lines\n",
      "Scraped 678 lines\n",
      "Scraped 714 lines\n",
      "Scraped 747 lines\n",
      "Scraped 783 lines\n",
      "Scraped 786 lines\n",
      "Finished working with parenting category. Scraped 786 lines\n",
      "\n",
      "786 lines of parenting category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with realestate category\n",
      "Number of pages = 7\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 141 lines\n",
      "Scraped 174 lines\n",
      "Scraped 207 lines\n",
      "Scraped 234 lines\n",
      "Finished working with realestate category. Scraped 234 lines\n",
      "\n",
      "234 lines of realestate category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with career category\n",
      "Number of pages = 68\n",
      "Scraped 36 lines\n",
      "Scraped 66 lines\n",
      "Scraped 102 lines\n",
      "Scraped 138 lines\n",
      "Scraped 174 lines\n",
      "Scraped 210 lines\n",
      "Scraped 246 lines\n",
      "Scraped 282 lines\n",
      "Scraped 318 lines\n",
      "Scraped 354 lines\n",
      "Scraped 390 lines\n",
      "Scraped 426 lines\n",
      "Scraped 462 lines\n",
      "Scraped 498 lines\n",
      "Scraped 534 lines\n",
      "Scraped 570 lines\n",
      "Scraped 606 lines\n",
      "Scraped 642 lines\n",
      "Scraped 678 lines\n",
      "Scraped 714 lines\n",
      "Scraped 750 lines\n",
      "Scraped 774 lines\n",
      "Scraped 801 lines\n",
      "Scraped 834 lines\n",
      "Scraped 870 lines\n",
      "Scraped 906 lines\n",
      "Scraped 939 lines\n",
      "Scraped 972 lines\n",
      "Scraped 1008 lines\n",
      "Scraped 1044 lines\n",
      "Scraped 1080 lines\n",
      "Scraped 1116 lines\n",
      "Scraped 1152 lines\n",
      "Scraped 1188 lines\n",
      "Scraped 1221 lines\n",
      "Scraped 1257 lines\n",
      "Scraped 1293 lines\n",
      "Scraped 1326 lines\n",
      "Scraped 1356 lines\n",
      "Scraped 1392 lines\n",
      "Scraped 1425 lines\n",
      "Scraped 1458 lines\n",
      "Scraped 1488 lines\n",
      "Scraped 1521 lines\n",
      "Scraped 1545 lines\n",
      "Scraped 1575 lines\n",
      "Scraped 1608 lines\n",
      "Scraped 1644 lines\n",
      "Scraped 1677 lines\n",
      "Scraped 1713 lines\n",
      "Scraped 1749 lines\n",
      "Scraped 1785 lines\n",
      "Scraped 1821 lines\n",
      "Scraped 1854 lines\n",
      "Scraped 1887 lines\n",
      "Scraped 1923 lines\n",
      "Scraped 1959 lines\n",
      "Scraped 1995 lines\n",
      "Scraped 2031 lines\n",
      "Scraped 2064 lines\n",
      "Scraped 2097 lines\n",
      "Scraped 2130 lines\n",
      "Scraped 2166 lines\n",
      "Scraped 2199 lines\n",
      "Scraped 2232 lines\n",
      "Scraped 2265 lines\n",
      "Scraped 2283 lines\n",
      "Scraped 2304 lines\n",
      "Finished working with career category. Scraped 2304 lines\n",
      "\n",
      "2304 lines of career category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with home category\n",
      "Number of pages = 10\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 141 lines\n",
      "Scraped 177 lines\n",
      "Scraped 213 lines\n",
      "Scraped 246 lines\n",
      "Scraped 282 lines\n",
      "Scraped 318 lines\n",
      "Scraped 330 lines\n",
      "Finished working with home category. Scraped 330 lines\n",
      "\n",
      "330 lines of home category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with nature category\n",
      "Number of pages = 13\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 108 lines\n",
      "Scraped 144 lines\n",
      "Scraped 180 lines\n",
      "Scraped 216 lines\n",
      "Scraped 252 lines\n",
      "Scraped 288 lines\n",
      "Scraped 324 lines\n",
      "Scraped 360 lines\n",
      "Scraped 396 lines\n",
      "Scraped 432 lines\n",
      "Scraped 468 lines\n",
      "Finished working with nature category. Scraped 468 lines\n",
      "\n",
      "468 lines of nature category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category, path in categories.items():\n",
    "    print(f'Working with {category} category')\n",
    "\n",
    "    lines = []\n",
    "    page_num = 1\n",
    "\n",
    "    while True:\n",
    "        url = f'{website}/{path}?page={page_num}#page'\n",
    "        page = requests.get(url)    \n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        if page_num == 1:\n",
    "            try:\n",
    "                limit = int(soup.find('div', class_='_2fQ8qDhH _1deHe_oF').\n",
    "                            find_all('a', class_='page-loader-btn ShimA1UP')[-1].text)\n",
    "            except AttributeError:\n",
    "                limit = 1\n",
    "            print(f'Number of pages = {limit}')\n",
    "            \n",
    "        divs = soup.find_all('div', class_='_23Qt4XY-')\n",
    "        if divs == []:\n",
    "            break\n",
    "\n",
    "        for div in divs[1:]:\n",
    "            sub_divs = div.find_all('a', class_='_2p02b0mw teaser-m-default')\n",
    "\n",
    "            for sub_div in sub_divs:\n",
    "                teaser_headline = sub_div.find('div', class_='_2QZpcT0M _3du5gVzi _3iAkatGy')\n",
    "                teaser = sub_div.find('div', class_='_2QcjsKyb M6MeFidn')\n",
    "                \n",
    "                if None in (teaser_headline, teaser):\n",
    "                    continue\n",
    "                \n",
    "                # going to the page of the article to get keywords\n",
    "                href = sub_div['href']\n",
    "                try:\n",
    "                    article = requests.get(website + href)\n",
    "                    article_soup = BeautifulSoup(article.content, 'html.parser')\n",
    "                    keywords = ' '.join(article_soup.find('meta', attrs={'name':'news_keywords'})['content'].split(','))\n",
    "                except TypeError:\n",
    "                    keywords = ''\n",
    "\n",
    "                teaser_headline = teaser_headline.text.strip()\n",
    "                teaser = teaser.text.strip()[:-5] #[:-5] to throw away the word \"Mehr\"\n",
    "                \n",
    "                lines.append(teaser_headline)\n",
    "                lines.append(teaser)\n",
    "                lines.append(keywords)\n",
    "\n",
    "        print(f'Scraped {len(lines)} lines')\n",
    "\n",
    "        page_num += 1\n",
    "        if page_num > limit:\n",
    "            break\n",
    "            \n",
    "    print(f'Finished working with {category} category. Scraped {len(lines)} lines\\n')\n",
    "    write_to_file(lines, category, 'beobachter_ch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
