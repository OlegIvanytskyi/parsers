{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'pets' : 'familie/haustiere', 'politics' : 'politik',\n",
    "              'economy' : 'wirtschaft', 'travel' : 'konsum/reisen',\n",
    "              'health' : 'gesundheit/medizin-krankheit', 'food' : 'ernahrung/lebensmittel',\n",
    "              'parenting' : 'familie/erziehung', 'realestate' : 'wohnen/eigentum',\n",
    "              'career' : 'arbeit', 'home' : 'wohnen/bauen-renovieren',\n",
    "              'nature' : 'umwelt/umweltpolitik'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with pets category\n",
      "Number of pages = 10\n",
      "Scraped 36 lines\n",
      "Scraped 72 lines\n",
      "Scraped 105 lines\n",
      "Scraped 141 lines\n",
      "Scraped 177 lines\n",
      "Scraped 213 lines\n",
      "Scraped 249 lines\n",
      "Scraped 285 lines\n",
      "Scraped 321 lines\n",
      "Scraped 324 lines\n",
      "Finished working with pets category. Scraped 324 lines\n",
      "\n",
      "324 lines of pets category have been scraped from beobachter_ch\n",
      "\n",
      "______________________________________________________________________________\n",
      "Working with politics category\n",
      "Number of pages = 17\n",
      "Scraped 24 lines\n",
      "Scraped 60 lines\n",
      "Scraped 96 lines\n",
      "Scraped 132 lines\n",
      "Scraped 168 lines\n",
      "Scraped 204 lines\n",
      "Scraped 240 lines\n",
      "Scraped 276 lines\n",
      "Scraped 312 lines\n",
      "Scraped 348 lines\n",
      "Scraped 384 lines\n",
      "Scraped 420 lines\n"
     ]
    }
   ],
   "source": [
    "website = 'https://www.beobachter.ch'\n",
    "\n",
    "for category, path in categories.items():\n",
    "    print(f'Working with {category} category')\n",
    "\n",
    "    lines = []\n",
    "    page_num = 1\n",
    "\n",
    "    while True:\n",
    "        url = f'{website}/{path}?page={page_num}#page'\n",
    "        page = requests.get(url)    \n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        if page_num == 1:\n",
    "            try:\n",
    "                limit = int(soup.find('div', class_='_2fQ8qDhH _1deHe_oF').\n",
    "                            find_all('a', class_='page-loader-btn ShimA1UP')[-1].text)\n",
    "                print(f'Number of pages = {limit}')\n",
    "            except AttributeError:\n",
    "                limit = 1\n",
    "                \n",
    "        \n",
    "        divs = soup.find_all('div', class_='_23Qt4XY-')\n",
    "\n",
    "        if divs == []:\n",
    "            break\n",
    "\n",
    "        for div in divs[1:]:\n",
    "            sub_divs = div.find_all('a', class_='_2p02b0mw teaser-m-default')\n",
    "\n",
    "            for sub_div in sub_divs:\n",
    "                teaser_headline = sub_div.find('div', class_='_2QZpcT0M _3du5gVzi _3iAkatGy')\n",
    "                teaser = sub_div.find('div', class_='_2QcjsKyb M6MeFidn')\n",
    "                href = sub_div['href']\n",
    "                \n",
    "                # going to the page of the article to get keywords\n",
    "                try:\n",
    "                    article = requests.get(website + href)\n",
    "                    article_soup = BeautifulSoup(article.content, 'html.parser')\n",
    "                    keywords = ' '.join(article_soup.find('meta', attrs={'name':'news_keywords'})['content'].split(','))\n",
    "                except TypeError:\n",
    "                    keywords = ''\n",
    "                    \n",
    "                if None in (teaser_headline, teaser):\n",
    "                    continue\n",
    "\n",
    "                teaser_headline = teaser_headline.text.strip()\n",
    "                teaser = teaser.text.strip()[:-5] #[:-5] to throw away the word \"Mehr\"\n",
    "                \n",
    "                lines.append(teaser_headline)\n",
    "                lines.append(teaser)\n",
    "                lines.append(keywords)\n",
    "\n",
    "        print(f'Scraped {len(lines)} lines')\n",
    "\n",
    "        page_num += 1\n",
    "        if page_num > limit:\n",
    "            break\n",
    "            \n",
    "    print(f'Finished working with {category} category. Scraped {len(lines)} lines\\n')\n",
    "    write_to_file(lines, category, 'beobachter_ch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
