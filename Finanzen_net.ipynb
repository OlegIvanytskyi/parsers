{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'career' : 'karriere', 'travel' : 'reise',\n",
    "              'realestate' : 'immobilien'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with career category\n",
      "Number of pages: 9\n",
      "Scraped 50 lines\n",
      "Scraped 100 lines\n",
      "Scraped 150 lines\n",
      "Scraped 200 lines\n",
      "Scraped 250 lines\n",
      "Scraped 300 lines\n",
      "Scraped 350 lines\n",
      "Scraped 400 lines\n",
      "Scraped 444 lines\n",
      "Finished working with career category. Scraped 444 lines\n",
      "\n",
      "444 lines of career category have been scraped from finanzen_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with travel category\n",
      "Number of pages: 6\n",
      "Scraped 50 lines\n",
      "Scraped 100 lines\n",
      "Scraped 150 lines\n",
      "Scraped 200 lines\n",
      "Scraped 250 lines\n",
      "Scraped 252 lines\n",
      "Finished working with travel category. Scraped 252 lines\n",
      "\n",
      "252 lines of travel category have been scraped from finanzen_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with realestate category\n",
      "Number of pages: 40\n",
      "Scraped 48 lines\n",
      "Scraped 98 lines\n",
      "Scraped 146 lines\n",
      "Scraped 190 lines\n",
      "Scraped 232 lines\n",
      "Scraped 282 lines\n",
      "Scraped 328 lines\n",
      "Scraped 378 lines\n",
      "Scraped 428 lines\n",
      "Scraped 476 lines\n",
      "Scraped 526 lines\n",
      "Scraped 574 lines\n",
      "Scraped 624 lines\n",
      "Scraped 672 lines\n",
      "Scraped 722 lines\n",
      "Scraped 770 lines\n",
      "Scraped 818 lines\n",
      "Scraped 868 lines\n",
      "Scraped 916 lines\n",
      "Scraped 964 lines\n",
      "Scraped 1012 lines\n",
      "Scraped 1060 lines\n",
      "Scraped 1108 lines\n",
      "Scraped 1156 lines\n",
      "Scraped 1206 lines\n",
      "Scraped 1256 lines\n",
      "Scraped 1306 lines\n",
      "Scraped 1356 lines\n",
      "Scraped 1406 lines\n",
      "Scraped 1454 lines\n",
      "Scraped 1500 lines\n",
      "Scraped 1548 lines\n",
      "Scraped 1596 lines\n",
      "Scraped 1642 lines\n",
      "Scraped 1682 lines\n",
      "Scraped 1726 lines\n",
      "Scraped 1774 lines\n",
      "Scraped 1822 lines\n",
      "Scraped 1866 lines\n",
      "Scraped 1916 lines\n",
      "Finished working with realestate category. Scraped 1916 lines\n",
      "\n",
      "1916 lines of realestate category have been scraped from finanzen_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category, path in categories.items():\n",
    "    print(f'Working with {category} category')\n",
    "\n",
    "    lines = []\n",
    "    page_num = 1\n",
    "\n",
    "    while True:\n",
    "        url = f'https://www.finanzen.net/nachrichten/rubrik/{path}@intpagenr_{page_num}'\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        if page_num == 1:\n",
    "            try:\n",
    "                limit = int(soup.find('div', class_='paging clear-block clearfix').find('a', class_='last').text)\n",
    "                print(f'Number of pages: {limit}')\n",
    "            except AttributeError:\n",
    "                limit = 1\n",
    "        \n",
    "        table = soup.find('table', class_='table news-list')\n",
    "\n",
    "        if table is None:\n",
    "            break\n",
    "\n",
    "        news_list = table.find_all('tr')\n",
    "        \n",
    "        for news in news_list:\n",
    "            teaser_headline = news.find('span', class_='teaser-headline')\n",
    "            teaser = news.find('div', class_='teaser')\n",
    "\n",
    "            if None in (teaser_headline, teaser):\n",
    "                continue\n",
    "\n",
    "            teaser_headline = teaser_headline.text.strip()\n",
    "            teaser = teaser.text.strip()[:-5] #[:-5] to throw away the word \"mehr\"\n",
    "            \n",
    "            lines.append(teaser_headline)\n",
    "            lines.append(teaser)\n",
    "\n",
    "        print(f'Scraped {len(lines)} lines')\n",
    "\n",
    "        page_num += 1\n",
    "        if page_num > limit:\n",
    "            break\n",
    "\n",
    "    print(f'Finished working with {category} category. Scraped {len(lines)} lines\\n')\n",
    "    write_to_file(lines, category, 'finanzen_net')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
