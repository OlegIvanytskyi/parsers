{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 84.0.4147\n",
      "[WDM] - Get LATEST driver version for 84.0.4147\n",
      "[WDM] - Driver [C:\\Users\\Oleg\\.wdm\\drivers\\chromedriver\\win32\\84.0.4147.30\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'auto' : ['autoindustrie'],\n",
    "              'incidents' : ['unfallforschung', 'mordfaelle', 'verkehrsunfaelle', \n",
    "                             'verkehrstote', 'sportunfaelle', 'explosionen'],\n",
    "              'sport' : ['basketball'],\n",
    "              'science' : ['erfindungen', 'akademiker'],\n",
    "              'health' : ['gesundheitssysteme', 'gesunde-ernaehrung', 'krankenhaeuser'],\n",
    "              'fashion' : ['models', 'modedesigner', 'mailaender-modewoche'],\n",
    "              'fitness' : ['fitnessstudio', 'fitnesstraining'],\n",
    "              'travel' : ['tourismus'],\n",
    "              'pets' : ['haustiere'], \n",
    "              'food' : ['essen-und-trinken', 'fast-food'],\n",
    "              'digital' : ['software', 'ios', 'android-apps', 'apps'], \n",
    "              'gaming' : ['computerspiele', 'xbox'],\n",
    "              'nature' : ['nachhaltigkeit-natur', 'klimaschutz', 'klimawandel'], \n",
    "              'parenting' : ['eltern', 'kindererziehung'],\n",
    "              'career' : ['karriere', 'arbeit-im-alter'], \n",
    "              'politics' : ['rassismus', 'brexit', 'donald-trump', 'wladimir-putin', \n",
    "                            'george-floyd', 'angela-merkel', 'boris-johnson'],\n",
    "              'cinema' : ['kinofilme', 'zeichentrickfilme'], \n",
    "              'realestate' : ['immobilienmarkt', 'immobilienkrise', 'immobilienfonds']}\n",
    "website = 'https://www.welt.de'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with cinema category\n",
      "Working with kinofilme path\n",
      "Number of pages: 32\n",
      "Scraped 147 lines\n",
      "Scraped 294 lines\n",
      "Scraped 441 lines\n",
      "Scraped 588 lines\n",
      "Scraped 735 lines\n",
      "Scraped 882 lines\n",
      "Scraped 1029 lines\n",
      "Scraped 1176 lines\n",
      "Scraped 1323 lines\n",
      "Scraped 1470 lines\n",
      "Scraped 1617 lines\n",
      "Scraped 1764 lines\n",
      "Scraped 1911 lines\n",
      "Scraped 2058 lines\n",
      "Scraped 2205 lines\n",
      "Scraped 2352 lines\n",
      "Scraped 2499 lines\n",
      "Scraped 2646 lines\n",
      "Scraped 2793 lines\n",
      "Scraped 2940 lines\n",
      "Scraped 3087 lines\n",
      "Scraped 3234 lines\n",
      "Scraped 3381 lines\n",
      "Scraped 3528 lines\n",
      "Scraped 3675 lines\n",
      "Scraped 3822 lines\n",
      "Scraped 3969 lines\n",
      "Scraped 4116 lines\n",
      "Scraped 4263 lines\n",
      "Scraped 4410 lines\n",
      "Scraped 4557 lines\n",
      "Scraped 4563 lines\n",
      "Working with zeichentrickfilme path\n",
      "Number of pages: 4\n",
      "Scraped 4710 lines\n",
      "Scraped 4857 lines\n",
      "Scraped 5004 lines\n",
      "Scraped 5049 lines\n",
      "Finished working with cinema category. Scraped 5049 lines\n",
      "\n",
      "5049 lines of cinema category have been scraped from welt_de\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with realestate category\n",
      "Working with immobilienmarkt path\n",
      "Number of pages: 25\n",
      "Scraped 147 lines\n",
      "Scraped 294 lines\n",
      "Scraped 441 lines\n",
      "Scraped 588 lines\n",
      "Scraped 735 lines\n",
      "Scraped 882 lines\n",
      "Scraped 1029 lines\n",
      "Scraped 1176 lines\n",
      "Scraped 1323 lines\n",
      "Scraped 1470 lines\n",
      "Scraped 1617 lines\n",
      "Scraped 1764 lines\n",
      "Scraped 1911 lines\n",
      "Scraped 2058 lines\n",
      "Scraped 2205 lines\n",
      "Scraped 2352 lines\n",
      "Scraped 2499 lines\n",
      "Scraped 2646 lines\n",
      "Scraped 2793 lines\n",
      "Scraped 2940 lines\n",
      "Scraped 3087 lines\n",
      "Scraped 3234 lines\n",
      "Scraped 3381 lines\n",
      "Scraped 3528 lines\n",
      "Scraped 3624 lines\n",
      "Working with immobilienkrise path\n",
      "Number of pages: 4\n",
      "Scraped 3771 lines\n",
      "Scraped 3912 lines\n",
      "Scraped 4053 lines\n",
      "Scraped 4167 lines\n",
      "Working with immobilienfonds path\n",
      "Number of pages: 1\n",
      "Scraped 4167 lines\n",
      "Finished working with realestate category. Scraped 4167 lines\n",
      "\n",
      "4167 lines of realestate category have been scraped from welt_de\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category, paths in categories.items():\n",
    "    print(f'Working with {category} category')\n",
    "\n",
    "    lines = []\n",
    "    \n",
    "    for path in paths:\n",
    "        print(f'Working with {path} path')\n",
    "        \n",
    "        page_num = 1\n",
    "\n",
    "        while True:\n",
    "            url = f'https://www.welt.de/themen/{path}/{page_num}/'\n",
    "            driver.get(url)\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            if page_num == 1:\n",
    "                try:\n",
    "                    limit = int(soup.find('ul', class_='c-pagination').find_all('li')[-2].text)\n",
    "                except (AttributeError, IndexError):\n",
    "                    limit = 1\n",
    "                print(f'Number of pages: {limit}')\n",
    "\n",
    "            main = soup.find('main', class_='c-page-container c-page-teaser-compact-view')\n",
    "            if main is None:\n",
    "                break\n",
    "\n",
    "            lis = main.find_all('li', class_='c-grid__item c-grid__item--is-one-third')\n",
    "\n",
    "            for li in lis:\n",
    "                teaser_headline = li.find('div', class_='c-dreifaltigkeit c-teaser-default__dreifaltigkeit') \\\n",
    "                                    .find('a', class_='o-link o-teaser__link o-teaser__link--is-headline')\n",
    "                teaser = li.find('div', class_='c-teaser-default__intro o-teaser__intro o-text')\n",
    "\n",
    "                if None in (teaser_headline, teaser):\n",
    "                    continue\n",
    "                    \n",
    "                # going to the page of the article to get keywords\n",
    "                href = teaser_headline['href']\n",
    "                try:\n",
    "                    article = driver.get(website + href)\n",
    "                    article_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    keywords = ' '.join(article_soup.find('meta', attrs={'name':'keywords'})['content'].split(','))\n",
    "                except TypeError:\n",
    "                    keywords = ''\n",
    "\n",
    "                teaser_headline = teaser_headline.text.strip()\n",
    "                teaser = teaser.text.strip()\n",
    "\n",
    "                lines.append(teaser_headline)\n",
    "                lines.append(teaser)\n",
    "                lines.append(keywords)\n",
    "\n",
    "            print(f'Scraped {len(lines)} lines')\n",
    "\n",
    "            page_num += 1\n",
    "            if page_num > limit:\n",
    "                break\n",
    "            \n",
    "    print(f'Finished working with {category} category. Scraped {len(lines)} lines\\n')\n",
    "    write_to_file(lines, category, 'welt_de')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
