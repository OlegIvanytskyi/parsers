{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'nature' : 'wissen/erde-klima', 'pets' : 'gesellschaft/tiere',\n",
    "              'tech' : 'technik-motor/digital', 'auto' : 'technik-motor/motor',\n",
    "              'sport' : 'sport/rhein-main-sport', 'fashion' : 'stil/mode-design',\n",
    "              'food' : 'stil/essen-trinken' , 'health' : 'gesellschaft/gesundheit',\n",
    "              'parenting' : 'feuilleton/familie', 'cinema' : 'feuilleton/kino',\n",
    "              'economy' : 'finanzen/finanzmarkt', 'home' : 'wirtschaft/wohnen/garten', \n",
    "              'politics' : 'politik/inland', 'career' : 'karriere-hochschule/buero-co'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with nature category\n",
      "nature category has 30 pages\n",
      "Scraped 60 articles\n",
      "Scraped 117 articles\n",
      "Scraped 177 articles\n",
      "Scraped 237 articles\n",
      "Scraped 294 articles\n",
      "Scraped 351 articles\n",
      "Scraped 411 articles\n",
      "Scraped 471 articles\n",
      "Scraped 531 articles\n",
      "Scraped 591 articles\n",
      "Scraped 651 articles\n",
      "Scraped 711 articles\n",
      "Scraped 771 articles\n",
      "Scraped 831 articles\n",
      "Scraped 885 articles\n",
      "Scraped 942 articles\n",
      "Scraped 1002 articles\n",
      "Scraped 1062 articles\n",
      "Scraped 1122 articles\n",
      "Scraped 1182 articles\n",
      "Scraped 1242 articles\n",
      "Scraped 1299 articles\n",
      "Scraped 1353 articles\n",
      "Scraped 1413 articles\n",
      "Scraped 1473 articles\n",
      "Scraped 1533 articles\n",
      "Scraped 1593 articles\n",
      "Scraped 1650 articles\n",
      "Scraped 1707 articles\n",
      "Scraped 1725 articles\n",
      "Finished working with nature category. Scraped 1725 lines\n",
      "1725 lines of nature category have been scraped from faz_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with pets category\n",
      "pets category has 51 pages\n",
      "Scraped 60 articles\n",
      "Scraped 120 articles\n",
      "Scraped 180 articles\n",
      "Scraped 240 articles\n",
      "Scraped 300 articles\n",
      "Scraped 360 articles\n",
      "Scraped 420 articles\n",
      "Scraped 480 articles\n",
      "Scraped 540 articles\n",
      "Scraped 600 articles\n",
      "Scraped 660 articles\n",
      "Scraped 720 articles\n",
      "Scraped 780 articles\n",
      "Scraped 840 articles\n",
      "Scraped 900 articles\n",
      "Scraped 960 articles\n",
      "Scraped 1020 articles\n",
      "Scraped 1080 articles\n",
      "Scraped 1140 articles\n",
      "Scraped 1200 articles\n",
      "Scraped 1260 articles\n",
      "Scraped 1320 articles\n",
      "Scraped 1380 articles\n",
      "Scraped 1440 articles\n",
      "Scraped 1500 articles\n",
      "Scraped 1560 articles\n",
      "Scraped 1620 articles\n",
      "Scraped 1680 articles\n",
      "Scraped 1740 articles\n",
      "Scraped 1800 articles\n",
      "Scraped 1860 articles\n",
      "Scraped 1920 articles\n",
      "Scraped 1980 articles\n",
      "Scraped 2040 articles\n",
      "Scraped 2094 articles\n",
      "Scraped 2154 articles\n",
      "Scraped 2214 articles\n",
      "Scraped 2271 articles\n",
      "Scraped 2325 articles\n",
      "Scraped 2382 articles\n",
      "Scraped 2442 articles\n",
      "Scraped 2502 articles\n",
      "Scraped 2562 articles\n",
      "Scraped 2616 articles\n",
      "Scraped 2670 articles\n",
      "Scraped 2730 articles\n",
      "Scraped 2790 articles\n",
      "Scraped 2850 articles\n",
      "Scraped 2910 articles\n",
      "Scraped 2970 articles\n",
      "Scraped 2985 articles\n",
      "Finished working with pets category. Scraped 2985 lines\n",
      "2985 lines of pets category have been scraped from faz_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with tech category\n",
      "tech category has 34 pages\n",
      "Scraped 60 articles\n",
      "Scraped 120 articles\n",
      "Scraped 180 articles\n",
      "Scraped 240 articles\n",
      "Scraped 300 articles\n",
      "Scraped 360 articles\n",
      "Scraped 420 articles\n",
      "Scraped 480 articles\n",
      "Scraped 540 articles\n",
      "Scraped 600 articles\n",
      "Scraped 660 articles\n",
      "Scraped 720 articles\n",
      "Scraped 780 articles\n",
      "Scraped 840 articles\n",
      "Scraped 900 articles\n",
      "Scraped 960 articles\n",
      "Scraped 1020 articles\n",
      "Scraped 1080 articles\n",
      "Scraped 1140 articles\n",
      "Scraped 1200 articles\n",
      "Scraped 1260 articles\n",
      "Scraped 1308 articles\n",
      "Scraped 1359 articles\n",
      "Scraped 1413 articles\n",
      "Scraped 1452 articles\n",
      "Scraped 1509 articles\n",
      "Scraped 1563 articles\n",
      "Scraped 1620 articles\n",
      "Scraped 1680 articles\n",
      "Scraped 1740 articles\n",
      "Scraped 1800 articles\n",
      "Scraped 1860 articles\n",
      "Scraped 1920 articles\n",
      "Scraped 1929 articles\n",
      "Finished working with tech category. Scraped 1929 lines\n",
      "1929 lines of tech category have been scraped from faz_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with auto category\n",
      "auto category has 51 pages\n",
      "Scraped 60 articles\n",
      "Scraped 120 articles\n",
      "Scraped 180 articles\n",
      "Scraped 240 articles\n",
      "Scraped 300 articles\n",
      "Scraped 360 articles\n",
      "Scraped 420 articles\n",
      "Scraped 480 articles\n",
      "Scraped 540 articles\n",
      "Scraped 600 articles\n",
      "Scraped 657 articles\n",
      "Scraped 717 articles\n",
      "Scraped 777 articles\n",
      "Scraped 837 articles\n",
      "Scraped 897 articles\n",
      "Scraped 957 articles\n",
      "Scraped 1017 articles\n",
      "Scraped 1077 articles\n",
      "Scraped 1137 articles\n",
      "Scraped 1197 articles\n",
      "Scraped 1257 articles\n",
      "Scraped 1317 articles\n",
      "Scraped 1377 articles\n",
      "Scraped 1437 articles\n",
      "Scraped 1497 articles\n",
      "Scraped 1557 articles\n",
      "Scraped 1617 articles\n",
      "Scraped 1677 articles\n",
      "Scraped 1737 articles\n",
      "Scraped 1797 articles\n",
      "Scraped 1857 articles\n",
      "Scraped 1917 articles\n",
      "Scraped 1977 articles\n",
      "Scraped 2037 articles\n",
      "Scraped 2097 articles\n",
      "Scraped 2157 articles\n",
      "Scraped 2217 articles\n",
      "Scraped 2277 articles\n",
      "Scraped 2337 articles\n",
      "Scraped 2397 articles\n",
      "Scraped 2457 articles\n",
      "Scraped 2517 articles\n",
      "Scraped 2577 articles\n",
      "Scraped 2628 articles\n",
      "Scraped 2685 articles\n",
      "Scraped 2745 articles\n",
      "Scraped 2805 articles\n",
      "Scraped 2865 articles\n",
      "Scraped 2925 articles\n",
      "Scraped 2982 articles\n",
      "Scraped 3000 articles\n",
      "Finished working with auto category. Scraped 3000 lines\n",
      "3000 lines of auto category have been scraped from faz_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with sport category\n",
      "sport category has 50 pages\n",
      "Scraped 60 articles\n",
      "Scraped 120 articles\n",
      "Scraped 180 articles\n",
      "Scraped 240 articles\n",
      "Scraped 300 articles\n",
      "Scraped 360 articles\n",
      "Scraped 420 articles\n",
      "Scraped 480 articles\n",
      "Scraped 540 articles\n",
      "Scraped 600 articles\n",
      "Scraped 660 articles\n",
      "Scraped 720 articles\n",
      "Scraped 780 articles\n",
      "Scraped 840 articles\n",
      "Scraped 900 articles\n",
      "Scraped 960 articles\n",
      "Scraped 1020 articles\n",
      "Scraped 1080 articles\n",
      "Scraped 1140 articles\n",
      "Scraped 1200 articles\n",
      "Scraped 1260 articles\n",
      "Scraped 1320 articles\n",
      "Scraped 1380 articles\n",
      "Scraped 1440 articles\n",
      "Scraped 1500 articles\n",
      "Scraped 1560 articles\n",
      "Scraped 1620 articles\n",
      "Scraped 1680 articles\n",
      "Scraped 1740 articles\n",
      "Scraped 1800 articles\n",
      "Scraped 1860 articles\n",
      "Scraped 1920 articles\n",
      "Scraped 1980 articles\n",
      "Scraped 2040 articles\n",
      "Scraped 2100 articles\n",
      "Scraped 2160 articles\n",
      "Scraped 2220 articles\n",
      "Scraped 2280 articles\n",
      "Scraped 2340 articles\n",
      "Scraped 2400 articles\n",
      "Scraped 2460 articles\n",
      "Scraped 2520 articles\n",
      "Scraped 2580 articles\n",
      "Scraped 2640 articles\n",
      "Scraped 2700 articles\n",
      "Scraped 2760 articles\n",
      "Scraped 2820 articles\n",
      "Scraped 2880 articles\n",
      "Scraped 2940 articles\n",
      "Scraped 3000 articles\n",
      "Finished working with sport category. Scraped 3000 lines\n",
      "3000 lines of sport category have been scraped from faz_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with fashion category\n",
      "fashion category has 51 pages\n",
      "Scraped 60 articles\n",
      "Scraped 120 articles\n",
      "Scraped 180 articles\n",
      "Scraped 240 articles\n",
      "Scraped 300 articles\n",
      "Scraped 360 articles\n",
      "Scraped 420 articles\n",
      "Scraped 480 articles\n",
      "Scraped 540 articles\n",
      "Scraped 600 articles\n",
      "Scraped 660 articles\n",
      "Scraped 720 articles\n",
      "Scraped 780 articles\n",
      "Scraped 840 articles\n",
      "Scraped 900 articles\n",
      "Scraped 960 articles\n",
      "Scraped 1020 articles\n",
      "Scraped 1080 articles\n",
      "Scraped 1140 articles\n",
      "Scraped 1200 articles\n",
      "Scraped 1260 articles\n",
      "Scraped 1320 articles\n",
      "Scraped 1380 articles\n",
      "Scraped 1440 articles\n",
      "Scraped 1500 articles\n",
      "Scraped 1560 articles\n",
      "Scraped 1620 articles\n",
      "Scraped 1680 articles\n",
      "Scraped 1740 articles\n",
      "Scraped 1800 articles\n",
      "Scraped 1860 articles\n",
      "Scraped 1920 articles\n",
      "Scraped 1980 articles\n",
      "Scraped 2040 articles\n",
      "Scraped 2100 articles\n",
      "Scraped 2160 articles\n",
      "Scraped 2220 articles\n",
      "Scraped 2280 articles\n",
      "Scraped 2340 articles\n",
      "Scraped 2400 articles\n",
      "Scraped 2460 articles\n",
      "Scraped 2520 articles\n",
      "Scraped 2580 articles\n",
      "Scraped 2640 articles\n",
      "Scraped 2700 articles\n",
      "Scraped 2757 articles\n",
      "Scraped 2817 articles\n",
      "Scraped 2877 articles\n",
      "Scraped 2937 articles\n",
      "Scraped 2997 articles\n",
      "Scraped 3000 articles\n",
      "Finished working with fashion category. Scraped 3000 lines\n",
      "3000 lines of fashion category have been scraped from faz_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with food category\n",
      "food category has 51 pages\n",
      "Scraped 60 articles\n",
      "Scraped 120 articles\n",
      "Scraped 180 articles\n",
      "Scraped 240 articles\n",
      "Scraped 300 articles\n",
      "Scraped 360 articles\n",
      "Scraped 420 articles\n",
      "Scraped 480 articles\n",
      "Scraped 540 articles\n",
      "Scraped 600 articles\n",
      "Scraped 660 articles\n",
      "Scraped 720 articles\n",
      "Scraped 780 articles\n",
      "Scraped 834 articles\n",
      "Scraped 894 articles\n",
      "Scraped 954 articles\n",
      "Scraped 1014 articles\n",
      "Scraped 1074 articles\n",
      "Scraped 1134 articles\n",
      "Scraped 1194 articles\n",
      "Scraped 1254 articles\n",
      "Scraped 1314 articles\n",
      "Scraped 1374 articles\n",
      "Scraped 1434 articles\n",
      "Scraped 1494 articles\n",
      "Scraped 1554 articles\n",
      "Scraped 1614 articles\n",
      "Scraped 1674 articles\n",
      "Scraped 1734 articles\n",
      "Scraped 1794 articles\n",
      "Scraped 1854 articles\n",
      "Scraped 1914 articles\n",
      "Scraped 1974 articles\n",
      "Scraped 2034 articles\n",
      "Scraped 2094 articles\n",
      "Scraped 2154 articles\n",
      "Scraped 2214 articles\n",
      "Scraped 2274 articles\n",
      "Scraped 2334 articles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 2394 articles\n",
      "Scraped 2454 articles\n",
      "Scraped 2514 articles\n",
      "Scraped 2574 articles\n",
      "Scraped 2631 articles\n",
      "Scraped 2688 articles\n",
      "Scraped 2748 articles\n",
      "Scraped 2805 articles\n",
      "Scraped 2865 articles\n",
      "Scraped 2925 articles\n",
      "Scraped 2985 articles\n",
      "Scraped 3000 articles\n",
      "Finished working with food category. Scraped 3000 lines\n",
      "3000 lines of food category have been scraped from faz_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with health category\n",
      "health category has 51 pages\n",
      "Scraped 60 articles\n",
      "Scraped 120 articles\n",
      "Scraped 180 articles\n",
      "Scraped 240 articles\n",
      "Scraped 300 articles\n",
      "Scraped 360 articles\n",
      "Scraped 420 articles\n",
      "Scraped 480 articles\n",
      "Scraped 540 articles\n",
      "Scraped 600 articles\n",
      "Scraped 660 articles\n",
      "Scraped 720 articles\n",
      "Scraped 780 articles\n",
      "Scraped 840 articles\n",
      "Scraped 900 articles\n",
      "Scraped 960 articles\n",
      "Scraped 1020 articles\n",
      "Scraped 1080 articles\n",
      "Scraped 1140 articles\n",
      "Scraped 1200 articles\n",
      "Scraped 1260 articles\n",
      "Scraped 1320 articles\n",
      "Scraped 1380 articles\n",
      "Scraped 1440 articles\n",
      "Scraped 1500 articles\n",
      "Scraped 1560 articles\n",
      "Scraped 1620 articles\n",
      "Scraped 1680 articles\n",
      "Scraped 1740 articles\n",
      "Scraped 1800 articles\n",
      "Scraped 1860 articles\n",
      "Scraped 1920 articles\n",
      "Scraped 1977 articles\n",
      "Scraped 2037 articles\n",
      "Scraped 2097 articles\n",
      "Scraped 2157 articles\n",
      "Scraped 2217 articles\n",
      "Scraped 2277 articles\n",
      "Scraped 2337 articles\n",
      "Scraped 2397 articles\n",
      "Scraped 2457 articles\n",
      "Scraped 2517 articles\n",
      "Scraped 2577 articles\n",
      "Scraped 2637 articles\n",
      "Scraped 2697 articles\n",
      "Scraped 2757 articles\n",
      "Scraped 2817 articles\n",
      "Scraped 2874 articles\n",
      "Scraped 2934 articles\n",
      "Scraped 2994 articles\n",
      "Scraped 3000 articles\n",
      "Finished working with health category. Scraped 3000 lines\n",
      "3000 lines of health category have been scraped from faz_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with parenting category\n",
      "parenting category has 51 pages\n",
      "Scraped 60 articles\n",
      "Scraped 120 articles\n",
      "Scraped 180 articles\n",
      "Scraped 240 articles\n",
      "Scraped 300 articles\n",
      "Scraped 360 articles\n",
      "Scraped 420 articles\n",
      "Scraped 480 articles\n",
      "Scraped 540 articles\n",
      "Scraped 600 articles\n",
      "Scraped 660 articles\n",
      "Scraped 720 articles\n",
      "Scraped 780 articles\n",
      "Scraped 840 articles\n",
      "Scraped 900 articles\n",
      "Scraped 960 articles\n",
      "Scraped 1020 articles\n",
      "Scraped 1080 articles\n",
      "Scraped 1140 articles\n",
      "Scraped 1200 articles\n",
      "Scraped 1260 articles\n",
      "Scraped 1320 articles\n",
      "Scraped 1380 articles\n",
      "Scraped 1440 articles\n",
      "Scraped 1500 articles\n",
      "Scraped 1560 articles\n",
      "Scraped 1620 articles\n",
      "Scraped 1680 articles\n",
      "Scraped 1740 articles\n",
      "Scraped 1800 articles\n",
      "Scraped 1860 articles\n",
      "Scraped 1920 articles\n",
      "Scraped 1980 articles\n",
      "Scraped 2040 articles\n",
      "Scraped 2100 articles\n",
      "Scraped 2160 articles\n",
      "Scraped 2220 articles\n",
      "Scraped 2280 articles\n",
      "Scraped 2340 articles\n",
      "Scraped 2400 articles\n",
      "Scraped 2460 articles\n",
      "Scraped 2517 articles\n",
      "Scraped 2577 articles\n",
      "Scraped 2637 articles\n",
      "Scraped 2697 articles\n",
      "Scraped 2757 articles\n",
      "Scraped 2814 articles\n",
      "Scraped 2874 articles\n",
      "Scraped 2934 articles\n",
      "Scraped 2994 articles\n",
      "Scraped 3000 articles\n",
      "Finished working with parenting category. Scraped 3000 lines\n",
      "3000 lines of parenting category have been scraped from faz_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with cinema category\n",
      "cinema category has 51 pages\n",
      "Scraped 60 articles\n",
      "Scraped 120 articles\n",
      "Scraped 180 articles\n",
      "Scraped 240 articles\n",
      "Scraped 300 articles\n",
      "Scraped 360 articles\n",
      "Scraped 420 articles\n",
      "Scraped 477 articles\n",
      "Scraped 537 articles\n",
      "Scraped 597 articles\n",
      "Scraped 657 articles\n",
      "Scraped 717 articles\n",
      "Scraped 777 articles\n",
      "Scraped 837 articles\n",
      "Scraped 897 articles\n",
      "Scraped 957 articles\n",
      "Scraped 1017 articles\n",
      "Scraped 1077 articles\n",
      "Scraped 1137 articles\n",
      "Scraped 1197 articles\n",
      "Scraped 1257 articles\n",
      "Scraped 1317 articles\n",
      "Scraped 1377 articles\n",
      "Scraped 1437 articles\n",
      "Scraped 1497 articles\n",
      "Scraped 1557 articles\n",
      "Scraped 1617 articles\n",
      "Scraped 1677 articles\n",
      "Scraped 1737 articles\n",
      "Scraped 1797 articles\n",
      "Scraped 1857 articles\n",
      "Scraped 1917 articles\n",
      "Scraped 1977 articles\n",
      "Scraped 2037 articles\n",
      "Scraped 2097 articles\n",
      "Scraped 2157 articles\n",
      "Scraped 2217 articles\n",
      "Scraped 2277 articles\n",
      "Scraped 2337 articles\n",
      "Scraped 2397 articles\n",
      "Scraped 2457 articles\n",
      "Scraped 2517 articles\n",
      "Scraped 2577 articles\n",
      "Scraped 2637 articles\n",
      "Scraped 2697 articles\n",
      "Scraped 2757 articles\n",
      "Scraped 2817 articles\n",
      "Scraped 2877 articles\n",
      "Scraped 2937 articles\n",
      "Scraped 2997 articles\n",
      "Scraped 3000 articles\n",
      "Finished working with cinema category. Scraped 3000 lines\n",
      "3000 lines of cinema category have been scraped from faz_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with economy category\n",
      "economy category has 51 pages\n",
      "Scraped 60 articles\n",
      "Scraped 120 articles\n",
      "Scraped 180 articles\n",
      "Scraped 237 articles\n",
      "Scraped 297 articles\n",
      "Scraped 357 articles\n",
      "Scraped 417 articles\n",
      "Scraped 477 articles\n",
      "Scraped 534 articles\n",
      "Scraped 594 articles\n",
      "Scraped 654 articles\n",
      "Scraped 714 articles\n",
      "Scraped 774 articles\n",
      "Scraped 834 articles\n",
      "Scraped 894 articles\n",
      "Scraped 954 articles\n",
      "Scraped 1014 articles\n",
      "Scraped 1074 articles\n",
      "Scraped 1134 articles\n",
      "Scraped 1194 articles\n",
      "Scraped 1254 articles\n",
      "Scraped 1314 articles\n",
      "Scraped 1374 articles\n",
      "Scraped 1434 articles\n",
      "Scraped 1494 articles\n",
      "Scraped 1554 articles\n",
      "Scraped 1614 articles\n",
      "Scraped 1674 articles\n",
      "Scraped 1734 articles\n",
      "Scraped 1794 articles\n",
      "Scraped 1854 articles\n",
      "Scraped 1914 articles\n",
      "Scraped 1974 articles\n",
      "Scraped 2034 articles\n",
      "Scraped 2094 articles\n",
      "Scraped 2154 articles\n",
      "Scraped 2214 articles\n",
      "Scraped 2274 articles\n",
      "Scraped 2334 articles\n",
      "Scraped 2394 articles\n",
      "Scraped 2454 articles\n",
      "Scraped 2514 articles\n",
      "Scraped 2571 articles\n",
      "Scraped 2628 articles\n",
      "Scraped 2688 articles\n",
      "Scraped 2748 articles\n",
      "Scraped 2808 articles\n",
      "Scraped 2868 articles\n",
      "Scraped 2928 articles\n",
      "Scraped 2988 articles\n",
      "Scraped 3000 articles\n",
      "Finished working with economy category. Scraped 3000 lines\n",
      "3000 lines of economy category have been scraped from faz_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with home category\n",
      "home category has 7 pages\n",
      "Scraped 60 articles\n",
      "Scraped 120 articles\n",
      "Scraped 177 articles\n",
      "Scraped 237 articles\n",
      "Scraped 297 articles\n",
      "Scraped 357 articles\n",
      "Scraped 387 articles\n",
      "Finished working with home category. Scraped 387 lines\n",
      "387 lines of home category have been scraped from faz_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with politics category\n",
      "politics category has 51 pages\n",
      "Scraped 60 articles\n",
      "Scraped 120 articles\n",
      "Scraped 180 articles\n",
      "Scraped 240 articles\n",
      "Scraped 300 articles\n",
      "Scraped 360 articles\n",
      "Scraped 420 articles\n",
      "Scraped 480 articles\n",
      "Scraped 540 articles\n",
      "Scraped 600 articles\n",
      "Scraped 660 articles\n",
      "Scraped 720 articles\n",
      "Scraped 780 articles\n",
      "Scraped 840 articles\n",
      "Scraped 900 articles\n",
      "Scraped 957 articles\n",
      "Scraped 1017 articles\n",
      "Scraped 1074 articles\n",
      "Scraped 1134 articles\n",
      "Scraped 1194 articles\n",
      "Scraped 1254 articles\n",
      "Scraped 1314 articles\n",
      "Scraped 1374 articles\n",
      "Scraped 1434 articles\n",
      "Scraped 1494 articles\n",
      "Scraped 1554 articles\n",
      "Scraped 1614 articles\n",
      "Scraped 1674 articles\n",
      "Scraped 1734 articles\n",
      "Scraped 1794 articles\n",
      "Scraped 1854 articles\n",
      "Scraped 1914 articles\n",
      "Scraped 1974 articles\n",
      "Scraped 2034 articles\n",
      "Scraped 2091 articles\n",
      "Scraped 2151 articles\n",
      "Scraped 2211 articles\n",
      "Scraped 2271 articles\n",
      "Scraped 2331 articles\n",
      "Scraped 2391 articles\n",
      "Scraped 2451 articles\n",
      "Scraped 2511 articles\n",
      "Scraped 2571 articles\n",
      "Scraped 2631 articles\n",
      "Scraped 2688 articles\n",
      "Scraped 2748 articles\n",
      "Scraped 2808 articles\n",
      "Scraped 2868 articles\n",
      "Scraped 2925 articles\n",
      "Scraped 2985 articles\n",
      "Scraped 3000 articles\n",
      "Finished working with politics category. Scraped 3000 lines\n",
      "3000 lines of politics category have been scraped from faz_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with career category\n",
      "career category has 51 pages\n",
      "Scraped 60 articles\n",
      "Scraped 120 articles\n",
      "Scraped 180 articles\n",
      "Scraped 240 articles\n",
      "Scraped 300 articles\n",
      "Scraped 360 articles\n",
      "Scraped 420 articles\n",
      "Scraped 480 articles\n",
      "Scraped 540 articles\n",
      "Scraped 600 articles\n",
      "Scraped 660 articles\n",
      "Scraped 720 articles\n",
      "Scraped 780 articles\n",
      "Scraped 840 articles\n",
      "Scraped 897 articles\n",
      "Scraped 957 articles\n",
      "Scraped 1017 articles\n",
      "Scraped 1077 articles\n",
      "Scraped 1137 articles\n",
      "Scraped 1197 articles\n",
      "Scraped 1257 articles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 1317 articles\n",
      "Scraped 1377 articles\n",
      "Scraped 1437 articles\n",
      "Scraped 1494 articles\n",
      "Scraped 1551 articles\n",
      "Scraped 1611 articles\n",
      "Scraped 1671 articles\n",
      "Scraped 1731 articles\n",
      "Scraped 1791 articles\n",
      "Scraped 1851 articles\n",
      "Scraped 1911 articles\n",
      "Scraped 1971 articles\n",
      "Scraped 2031 articles\n",
      "Scraped 2091 articles\n",
      "Scraped 2151 articles\n",
      "Scraped 2211 articles\n",
      "Scraped 2271 articles\n",
      "Scraped 2331 articles\n",
      "Scraped 2391 articles\n",
      "Scraped 2451 articles\n",
      "Scraped 2511 articles\n",
      "Scraped 2571 articles\n",
      "Scraped 2631 articles\n",
      "Scraped 2691 articles\n",
      "Scraped 2751 articles\n",
      "Scraped 2811 articles\n",
      "Scraped 2871 articles\n",
      "Scraped 2931 articles\n",
      "Scraped 2991 articles\n",
      "Scraped 3003 articles\n",
      "Finished working with career category. Scraped 3003 lines\n",
      "3003 lines of career category have been scraped from faz_net\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category, path in categories.items():\n",
    "    print(f'Working with {category} category')\n",
    "    \n",
    "    lines = []\n",
    "    page_num = 1\n",
    "    \n",
    "    while True:\n",
    "        url = f'https://www.faz.net/aktuell/{path}/s{page_num}.html'\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        if page_num == 1:\n",
    "            try:\n",
    "                limit = int(soup.find('ul', class_='nvg-Paginator nvg-Paginator-is-right'). \\\n",
    "                                 find_all('li', class_='nvg-Paginator_Item nvg-Paginator_Item-page-number')[-1].text)\n",
    "            except AttributeError:\n",
    "                limit = 1\n",
    "            print(f'{category} category has {limit} pages')\n",
    "            \n",
    "        div = soup.find('div', class_='Home')\n",
    "        if div is None:\n",
    "            break\n",
    "            \n",
    "        lis = div.find_all('li', class_='lst-Teaser_Item')\n",
    "        if lis == []:\n",
    "            break\n",
    "            \n",
    "        for li in lis:\n",
    "            try:\n",
    "                headline = li.find('a', class_='js-hlp-LinkSwap js-tsr-Base_ContentLink tsr-Base_ContentLink')\n",
    "                \n",
    "                teaser_headline = headline.find('span', class_='tsr-Base_HeadlineText')\n",
    "                teaser = li.find('div', class_='tsr-Base_Content')\n",
    "                \n",
    "                if None in (teaser_headline, teaser):\n",
    "                    continue\n",
    "                \n",
    "                # going to the page of the article to get keywords\n",
    "                href = headline['href']\n",
    "                try:\n",
    "                    article = requests.get(href)\n",
    "                    article_soup = BeautifulSoup(article.content, 'html.parser')\n",
    "                    keywords = ' '.join(article_soup.find('meta', attrs={'name':'keywords'})['content'].split(','))\n",
    "                except TypeError:\n",
    "                    keywords = ''\n",
    "                \n",
    "                teaser_headline = teaser_headline.text.strip()\n",
    "                teaser = teaser.text.strip()\n",
    "                \n",
    "                lines.append(teaser_headline)\n",
    "                lines.append(teaser)\n",
    "                lines.append(keywords)\n",
    "                \n",
    "            except AttributeError:\n",
    "                continue\n",
    "                \n",
    "        print(f'Scraped {len(lines)} articles')\n",
    "        \n",
    "        page_num += 1\n",
    "        if page_num > limit:\n",
    "            break\n",
    "        \n",
    "    print(f'Finished working with {category} category. Scraped {len(lines)} lines\\n')\n",
    "    write_to_file(lines, category, 'faz_net')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
