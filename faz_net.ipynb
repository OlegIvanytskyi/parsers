{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'nature' : 'wissen/erde-klima', 'pets' : 'gesellschaft/tiere',\n",
    "              'tech' : 'technik-motor/digital', 'auto' : 'technik-motor/motor',\n",
    "              'sport' : 'sport/rhein-main-sport', 'fashion' : 'stil/mode-design',\n",
    "              'food' : 'stil/essen-trinken' , 'health' : 'gesellschaft/gesundheit',\n",
    "              'parenting' : 'feuilleton/familie', 'cinema' : 'feuilleton/kino',\n",
    "              'economy' : 'finanzen/finanzmarkt', 'home' : 'wirtschaft/wohnen/garten', \n",
    "              'politics' : 'politik/inland', 'career' : 'karriere-hochschule/buero-co'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with nature category\n",
      "nature category has 29 pages\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Finished working with nature category. Scraped 549 articles\n",
      "549 articles of nature category scraped from faz_net\n",
      "\n",
      "Working with pets category\n",
      "pets category has 47 pages\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Scraped 600 articles\n",
      "Scraped 650 articles\n",
      "Scraped 700 articles\n",
      "Scraped 750 articles\n",
      "Scraped 800 articles\n",
      "Scraped 850 articles\n",
      "Scraped 900 articles\n",
      "Finished working with pets category. Scraped 923 articles\n",
      "923 articles of pets category scraped from faz_net\n",
      "\n",
      "Working with tech category\n",
      "tech category has 32 pages\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Scraped 600 articles\n",
      "Finished working with tech category. Scraped 601 articles\n",
      "601 articles of tech category scraped from faz_net\n",
      "\n",
      "Working with auto category\n",
      "auto category has 51 pages\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Scraped 600 articles\n",
      "Scraped 650 articles\n",
      "Scraped 700 articles\n",
      "Scraped 750 articles\n",
      "Scraped 800 articles\n",
      "Scraped 850 articles\n",
      "Scraped 900 articles\n",
      "Scraped 950 articles\n",
      "Scraped 1000 articles\n",
      "Finished working with auto category. Scraped 1000 articles\n",
      "1000 articles of auto category scraped from faz_net\n",
      "\n",
      "Working with sport category\n",
      "sport category has 51 pages\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Scraped 600 articles\n",
      "Scraped 650 articles\n",
      "Scraped 700 articles\n",
      "Scraped 750 articles\n",
      "Scraped 800 articles\n",
      "Scraped 850 articles\n",
      "Scraped 900 articles\n",
      "Scraped 950 articles\n",
      "Scraped 1000 articles\n",
      "Finished working with sport category. Scraped 1000 articles\n",
      "1000 articles of sport category scraped from faz_net\n",
      "\n",
      "Working with fashion category\n",
      "fashion category has 51 pages\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Scraped 600 articles\n",
      "Scraped 650 articles\n",
      "Scraped 700 articles\n",
      "Scraped 750 articles\n",
      "Scraped 800 articles\n",
      "Scraped 850 articles\n",
      "Scraped 900 articles\n",
      "Scraped 950 articles\n",
      "Scraped 1000 articles\n",
      "Finished working with fashion category. Scraped 1000 articles\n",
      "1000 articles of fashion category scraped from faz_net\n",
      "\n",
      "Working with food category\n",
      "food category has 51 pages\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Scraped 600 articles\n",
      "Scraped 650 articles\n",
      "Scraped 700 articles\n",
      "Scraped 750 articles\n",
      "Scraped 800 articles\n",
      "Scraped 850 articles\n",
      "Scraped 900 articles\n",
      "Scraped 950 articles\n",
      "Scraped 1000 articles\n",
      "Finished working with food category. Scraped 1000 articles\n",
      "1000 articles of food category scraped from faz_net\n",
      "\n",
      "Working with health category\n",
      "health category has 51 pages\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Scraped 600 articles\n",
      "Scraped 650 articles\n",
      "Scraped 700 articles\n",
      "Scraped 750 articles\n",
      "Scraped 800 articles\n",
      "Scraped 850 articles\n",
      "Scraped 900 articles\n",
      "Scraped 950 articles\n",
      "Scraped 1000 articles\n",
      "Finished working with health category. Scraped 1000 articles\n",
      "1000 articles of health category scraped from faz_net\n",
      "\n",
      "Working with parenting category\n",
      "parenting category has 51 pages\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Scraped 600 articles\n",
      "Scraped 650 articles\n",
      "Scraped 700 articles\n",
      "Scraped 750 articles\n",
      "Scraped 800 articles\n",
      "Scraped 850 articles\n",
      "Scraped 900 articles\n",
      "Scraped 950 articles\n",
      "Scraped 1000 articles\n",
      "Finished working with parenting category. Scraped 1000 articles\n",
      "1000 articles of parenting category scraped from faz_net\n",
      "\n",
      "Working with cinema category\n",
      "cinema category has 51 pages\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Scraped 600 articles\n",
      "Scraped 650 articles\n",
      "Scraped 700 articles\n",
      "Scraped 750 articles\n",
      "Scraped 800 articles\n",
      "Scraped 850 articles\n",
      "Scraped 900 articles\n",
      "Scraped 950 articles\n",
      "Scraped 1000 articles\n",
      "Finished working with cinema category. Scraped 1000 articles\n",
      "1000 articles of cinema category scraped from faz_net\n",
      "\n",
      "Working with economy category\n",
      "economy category has 51 pages\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Scraped 600 articles\n",
      "Scraped 650 articles\n",
      "Scraped 700 articles\n",
      "Scraped 750 articles\n",
      "Scraped 800 articles\n",
      "Scraped 850 articles\n",
      "Scraped 900 articles\n",
      "Scraped 950 articles\n",
      "Scraped 1000 articles\n",
      "Finished working with economy category. Scraped 1000 articles\n",
      "1000 articles of economy category scraped from faz_net\n",
      "\n",
      "Working with home category\n",
      "home category has 6 pages\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Finished working with home category. Scraped 117 articles\n",
      "117 articles of home category scraped from faz_net\n",
      "\n",
      "Working with politics category\n",
      "politics category has 51 pages\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Scraped 600 articles\n",
      "Scraped 650 articles\n",
      "Scraped 700 articles\n",
      "Scraped 750 articles\n",
      "Scraped 800 articles\n",
      "Scraped 850 articles\n",
      "Scraped 900 articles\n",
      "Scraped 950 articles\n",
      "Scraped 1000 articles\n",
      "Finished working with politics category. Scraped 1000 articles\n",
      "1000 articles of politics category scraped from faz_net\n",
      "\n",
      "Working with career category\n",
      "career category has 51 pages\n",
      "Scraped 50 articles\n",
      "Scraped 100 articles\n",
      "Scraped 150 articles\n",
      "Scraped 200 articles\n",
      "Scraped 250 articles\n",
      "Scraped 300 articles\n",
      "Scraped 350 articles\n",
      "Scraped 400 articles\n",
      "Scraped 450 articles\n",
      "Scraped 500 articles\n",
      "Scraped 550 articles\n",
      "Scraped 600 articles\n",
      "Scraped 650 articles\n",
      "Scraped 700 articles\n",
      "Scraped 750 articles\n",
      "Scraped 800 articles\n",
      "Scraped 850 articles\n",
      "Scraped 900 articles\n",
      "Scraped 950 articles\n",
      "Scraped 1000 articles\n",
      "Finished working with career category. Scraped 1000 articles\n",
      "1000 articles of career category scraped from faz_net\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category, path in categories.items():\n",
    "    print(f'Working with {category} category')\n",
    "    \n",
    "    articles = []\n",
    "    page_num = 1\n",
    "    \n",
    "    while True:\n",
    "        url = f'https://www.faz.net/aktuell/{path}/s{page_num}.html'\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        if page_num == 1:\n",
    "            limit = int(soup.find('ul', class_='nvg-Paginator nvg-Paginator-is-right'). \\\n",
    "                             find_all('li', class_='nvg-Paginator_Item nvg-Paginator_Item-page-number')[-1].text)\n",
    "            print(f'{category} category has {limit} pages')\n",
    "            \n",
    "        div = soup.find('div', class_='Home')\n",
    "        if div is None:\n",
    "            break\n",
    "            \n",
    "        lis = div.find_all('li', class_='lst-Teaser_Item')\n",
    "        if lis == []:\n",
    "            break\n",
    "            \n",
    "        for li in lis:\n",
    "            try:\n",
    "                teaser_headline = li.find('a', class_='js-hlp-LinkSwap js-tsr-Base_ContentLink tsr-Base_ContentLink'). \\\n",
    "                                     find('span', class_='tsr-Base_HeadlineText')\n",
    "                teaser = li.find('div', class_='tsr-Base_Content')\n",
    "                \n",
    "                if None in (teaser_headline, teaser):\n",
    "                    continue\n",
    "                \n",
    "                teaser_headline = teaser_headline.text.strip()\n",
    "                teaser = teaser.text.strip()\n",
    "                \n",
    "                articles.append([teaser_headline, teaser])\n",
    "                \n",
    "                if len(articles) % 50 == 0:\n",
    "                    print(f'Scraped {len(articles)} articles')\n",
    "            except AttributeError:\n",
    "                continue\n",
    "        \n",
    "        page_num += 1\n",
    "        if page_num > limit:\n",
    "            break\n",
    "        \n",
    "    print(f'Finished working with {category} category. Scraped {len(articles)} articles')\n",
    "    write_to_file(articles, category, 'faz_net')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
