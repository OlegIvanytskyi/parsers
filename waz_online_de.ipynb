{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'politics' : 'Nachrichten/Politik', 'health' : 'Mehr/Gesundheit',\n",
    "              'economy' : 'Nachrichten/Wirtschaft', 'tech' : 'Nachrichten/Digital',\n",
    "              'auto' : 'Mehr/Auto-Verkehr', 'sport' : 'Sportbuzzer/Fussball-ueberregional',\n",
    "              'home' : 'Mehr/Bauen-Wohnen', 'food' : 'Mehr/Essen-Trinken'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with politics category\n",
      "Number of pages: 4\n",
      "Scraped 22 lines\n",
      "Scraped 44 lines\n",
      "Scraped 66 lines\n",
      "Scraped 88 lines\n",
      "Finished working with politics category. Scraped 88 lines\n",
      "\n",
      "88 lines of politics category have been scraped from waz_online_de\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with health category\n",
      "Number of pages: 4\n",
      "Scraped 22 lines\n",
      "Scraped 44 lines\n",
      "Scraped 66 lines\n",
      "Scraped 88 lines\n",
      "Finished working with health category. Scraped 88 lines\n",
      "\n",
      "88 lines of health category have been scraped from waz_online_de\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with economy category\n",
      "Number of pages: 4\n",
      "Scraped 22 lines\n",
      "Scraped 44 lines\n",
      "Scraped 66 lines\n",
      "Scraped 88 lines\n",
      "Finished working with economy category. Scraped 88 lines\n",
      "\n",
      "88 lines of economy category have been scraped from waz_online_de\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with tech category\n",
      "Number of pages: 4\n",
      "Scraped 22 lines\n",
      "Scraped 44 lines\n",
      "Scraped 66 lines\n",
      "Scraped 88 lines\n",
      "Finished working with tech category. Scraped 88 lines\n",
      "\n",
      "88 lines of tech category have been scraped from waz_online_de\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with auto category\n",
      "Number of pages: 4\n",
      "Scraped 22 lines\n",
      "Scraped 44 lines\n",
      "Scraped 66 lines\n",
      "Scraped 88 lines\n",
      "Finished working with auto category. Scraped 88 lines\n",
      "\n",
      "88 lines of auto category have been scraped from waz_online_de\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with sport category\n",
      "Number of pages: 4\n",
      "Scraped 22 lines\n",
      "Scraped 44 lines\n",
      "Finished working with sport category. Scraped 44 lines\n",
      "\n",
      "44 lines of sport category have been scraped from waz_online_de\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with home category\n",
      "Number of pages: 4\n",
      "Scraped 22 lines\n",
      "Scraped 44 lines\n",
      "Scraped 66 lines\n",
      "Scraped 84 lines\n",
      "Finished working with home category. Scraped 84 lines\n",
      "\n",
      "84 lines of home category have been scraped from waz_online_de\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n",
      "Working with food category\n",
      "Number of pages: 4\n",
      "Scraped 22 lines\n",
      "Scraped 44 lines\n",
      "Scraped 66 lines\n",
      "Scraped 88 lines\n",
      "Finished working with food category. Scraped 88 lines\n",
      "\n",
      "88 lines of food category have been scraped from waz_online_de\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category, path in categories.items():\n",
    "    print(f'Working with {category} category')\n",
    "    \n",
    "    lines = []\n",
    "    page_num = 1\n",
    "    \n",
    "    while True:\n",
    "        url = f'https://www.waz-online.de/{path}/{page_num}#anchor'\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        if page_num == 1:\n",
    "            counter = soup.find('div', class_='pdb-pagepagination-navigation pdb-pagepagination-navigation_light')\n",
    "            if counter == None:\n",
    "                limit = 1\n",
    "            else:\n",
    "                limit = int(counter.find_all('a')[-3].text)\n",
    "            print(f'Number of pages: {limit}')\n",
    "        \n",
    "        div = soup.find('div', class_='pdb-articles12 row')\n",
    "        if div is None:\n",
    "            break\n",
    "            \n",
    "        rows = div.find_all('div', class_='pdb-teaser pdb-teaser3-row row')\n",
    "        if rows == []:\n",
    "            break\n",
    "            \n",
    "        for row in rows:\n",
    "            columns = row.find_all('div', class_='pdb-teaser3-row-item col-4')\n",
    "            if columns == []:\n",
    "                break\n",
    "            \n",
    "            for column in columns:\n",
    "                teaser_headline = column.find('div', class_='pdb-teaser3-teaser-breadcrumb')\n",
    "                teaser = column.find('div', class_='pdb-teaser3-teaser-intro')\n",
    "                \n",
    "                if None in (teaser_headline, teaser):\n",
    "                    continue\n",
    "                    \n",
    "                teaser_headline = teaser_headline.text.strip()\n",
    "                teaser = teaser.text.strip()\n",
    "                \n",
    "                lines.append(teaser_headline)\n",
    "                lines.append(teaser)\n",
    "                \n",
    "        print(f'Scraped {len(lines)} lines')\n",
    "                    \n",
    "        page_num += 1\n",
    "        if page_num > limit:\n",
    "            break\n",
    "    \n",
    "    print(f'Finished working with {category} category. Scraped {len(lines)} lines\\n')\n",
    "    write_to_file(lines, category, 'waz_online_de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another parser for this website (category hobbies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 10\n",
      "Scraped 16 lines\n",
      "Scraped 32 lines\n",
      "Scraped 48 lines\n",
      "Scraped 64 lines\n",
      "Scraped 80 lines\n",
      "Scraped 96 lines\n",
      "Scraped 112 lines\n",
      "Scraped 128 lines\n",
      "Scraped 144 lines\n",
      "Scraped 160 lines\n",
      "160 lines of hobbies category have been scraped from waz_online_de\n",
      "\n",
      "______________________________________________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = []\n",
    "page_num = 1\n",
    "\n",
    "while True:\n",
    "    url = f'https://www.waz-online.de/suche?query=hobby&in=title_intro&sort=article&date=-1&page={page_num}'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    if page_num == 1:\n",
    "        counter = soup.find('div', class_='pdb-pagepagination-navigation pdb-pagepagination-navigation_light')\n",
    "        if counter == None:\n",
    "            limit = 1\n",
    "        else:\n",
    "            limit = int(counter.find_all('a')[-3].text)\n",
    "        print(f'Number of pages: {limit}')\n",
    "\n",
    "    div = soup.find('div', class_='pdb-search-result')\n",
    "    if div is None:\n",
    "        break\n",
    "\n",
    "    rows = div.find_all('div', class_='pdb-teaser pdb-teaser3-row row')\n",
    "    if rows == []:\n",
    "        break\n",
    "\n",
    "    for row in rows:\n",
    "        columns = row.find_all('div', class_='pdb-teaser3-row-item col-4')\n",
    "        if columns == []:\n",
    "            break\n",
    "\n",
    "        for column in columns:\n",
    "            teaser_headline = column.find('div', class_='pdb-teaser3-teaser-breadcrumb')\n",
    "            teaser = column.find('div', class_='pdb-teaser3-teaser-intro')\n",
    "\n",
    "            if None in (teaser_headline, teaser):\n",
    "                continue\n",
    "\n",
    "            teaser_headline = teaser_headline.text.strip()\n",
    "            teaser = teaser.text.strip()\n",
    "\n",
    "            lines.append(teaser_headline)\n",
    "            lines.append(teaser)\n",
    "\n",
    "    print(f'Scraped {len(lines)} lines')\n",
    "\n",
    "    page_num += 1\n",
    "    if page_num > limit:\n",
    "        break\n",
    "\n",
    "write_to_file(lines, 'hobbies', 'waz_online_de')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
